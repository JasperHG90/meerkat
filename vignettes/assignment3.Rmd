---
title: "Assignment 3"
author: "Jasper Ginn"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{assignment2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

The code in this package is stored in the 'meerkat' package. It contains helper functions that are not exported and for which no visible documentation was written. At the start of each part, I give a link to the R-code hosted on GitHub where you can view the code.

## PART I

View the code belonging to this assignment [here](https://github.com/JasperHG90/meerkat/blob/master/R/a3_power.R)

*1. Assuming that the data are approximately normal, simulate one sample of data and perform a t-test using the R in-built t-test function. What is the p-value of the test? What is your conclusion?.*

```{r}
rm(list=ls())
library(meerkat)

# Set values and draw a sample
n <- 50
mu <- 160
mu0 <- 150
sd <- 15
set.seed(600)
x <- rnorm(n, mu, sd)
set.seed(500)
y <- rnorm(n, mu0, sd)
# We perform a t-test on the data
t.test(x, y, alternative="greater", var.equal = TRUE)
```

The one-tailed test is statistically significant (p<0.001). At a significance level of a=0.05 we would therefore reject the null that there is no difference in the test scores of these groups. The experimental group , it would seem, performed better than the control and the special training has had the desired effect.

**2. Assume that the data are normally distributed, write a function that generates data and performs a t-test 1000 times and that stores the values of the t-statistic (or the p-value) in a vector. Thinking about the definition of power in terms of rejecting the null hypothesis, how would you obtain an estimate of the power in this situation? Compare your results with those given by power.t.test**

*Power* is defined as the probability $\pi(\theta)$ of rejecting the null hypothesis given that the alternative hypothesis is true.^[Rizzo, Maria L. "Statistical Computing with R". Chapman and Hall/CRC, 2007]. The probability of making a Type II error (wrongfully concluding that there is no effect while an effect exists) is $1-\pi(\theta))$. A common maximum value for the Type II error is that we should not commit it in more than 20\% of cases. Hence, the 'minimum' power we should strive for is $0.8$, meaning that we can detect effects $4$ out of $5$ times. 

The function `emp_power()` calculates, for a given $R$ number of repititions, the t-statistic between the mean value of the control group and that of a random sample drawn from the normal distribution. Given that we are interested in the probability $\pi(\theta)$, a reasonable approach is to take the proportion of cases where the null hypothesis is rejected. In this case, if the true difference of means is $0$, we would still expect to achieve a power of 5\% because of the Type I error rate. As we increase the difference in mean values, we expect the power (or the percentage of significant t-tests) to increase.

Furthermore, given that we draw data from a normal distribution and for values where $\theta_0 < \theta_1$, we may be confident that the power of our test increases as we:

1. increase the sample size
2. increase the significance level $\alpha$ (e.g. from $0.05$ to $0.1$)
3. decrease the standard deviation in the sample

The effect of changing these values is shown in the plots below

```{r}
library(ggplot2)
library(grid)
library(gridExtra)
# Define a grid of possible mean values from 140 to 160
grids <- list(
  grid_mu = seq(140, 160, 1),
  grid_sd = seq(5, 15, 0.5),
  grid_alpha = seq(0.01, 0.11, 0.005),
  grid_n = seq(50, 150, 5)
)

grids_out <- vector("list", length = 4)
# For each value, calculate the power of the t-test while holding the other parameters constant
for( i in seq_along(grids) ) {
  
  # Get name
  grids_in_name <- names(grids)[i]
  grid <- grids[[i]]
  
  # This switch is a wrapper for a bunch of if/else statements
  power_values <- switch(grids_in_name,
                          grid_mu = lapply(grid, function(x) emp_power(50, 
                                                                       x, 
                                                                       15, 
                                                                       R=1000, 
                                                                       type="two_sample",
                                                                       alpha=0.05, 
                                                                       alternative="greater", 
                                                                       mu0=150)),
                          grid_n = lapply(grid, function(x) emp_power(x, 155, 15, R=1000, 
                                                                      type="two_sample", alpha=0.05, 
                                                                      alternative="greater", mu0=150)),
                          grid_sd = lapply(grid, function(x) emp_power(50, 155, x, R=1000, 
                                                                       type="two_sample",alpha=0.05, 
                                                                       alternative="greater", mu0=150)),
                          grid_alpha = lapply(grid, function(x) emp_power(50, 155, 15, R=1000, 
                                                                          type="two_sample",alpha=x, 
                                                                          alternative="greater", mu0=150))
  )
  
  # Get values
  pv <- sapply(power_values, function(x) x$power)
  
  # In data frame
  df <- data.frame("grid" = grid,
                   "power" = pv,
                   "se" = sapply(power_values, function(x) x$se))
  
  # Plot
  grids_out[[i]] <- ggplot(df, aes(x=grid, y=power)) +
    geom_line() +
    geom_point() +
    geom_errorbar(aes(ymin=power - se, ymax=power + se)) + 
    theme_bw() +
    scale_x_continuous(name = strsplit(grids_in_name, "_")[[1]][2]) +
    scale_y_continuous(name = "Power")
  
}

# Plot
grid.arrange(grids_out[[1]], grids_out[[2]],
             grids_out[[3]], grids_out[[4]], 
             ncol=2, nrow=2)

```

If we compare the results from the empirical test to the R function, we observe:

```{r}
power.t.test(n=50, delta=10, sd=15, type="two.sample", 
             sig.level = 0.05, alternative="one.sided")
```

and

```{r}
set.seed(700)
emp <- emp_power(n=50, mu=160, mu0=150, sd=15, 
                 alpha=0.05, type="two_sample",
                 alternative="greater", R=1000)
emp$power
```

This gives us a comparable power estimation.

## PART II

View the code belonging to this assignment [here](https://github.com/JasperHG90/meerkat/blob/master/R/a3_perm.R)

1. **Choose an appropriate resampling technique and make a function that performs a hypothesis test based on the t-statistic produced by the standard t-test function. Use the in-built sample() function to perform the resampling of the data. Include relevant statistics in your function and give the function a clear structure (choose sensible input arguments, organize the output in an efficient way, add comments, etc). Show the results of your resampling technique on the Flight instruction data.**

We begin by specifying the data

```{r}
# Create variables for flight simulation data
CSFI <- c(2,5,5,6,6,7,8,9)
TFI <- c(1,1,2,3,3,4,5,7,7,8)
```

```{r}
# Run
pt <- permutation_test(CSFI, TFI, 999)
pt
```

```{r}
plot(pt)
```

```{r}
t.test(CSFI, TFI)
```

2. **How would you program the drawing of random samples when you do not want to use the sample() function? Modify the function you have programmed in (1) in such a way that it does not use the sample() function anymore. Run your new function and show that you can obtain the same results as in (1).**

We can use the R function `runif()` to draw values from the uniform distribution. 

```{r}
set.seed(100)
# This draws 5 values from a uniform with a=0 and b=1
runif(5)
```

If we wanted 80% of our rows randomly sampled, then we subset the probabilities generated by the uniform distribution such that they are below 0.8. This works because the uniform distribution is linear. 

```{r}
# Sample from the density from 0 to 1
library(ggplot2)
library(gridExtra)
x <- seq(0, 1, 0.1)
df <- data.frame(x=x, dens=dunif(x, 0, 1), dist=punif(x,0,1), rand = runif(length(x), 0, 1))
p1 <- ggplot(df, aes(x=x, y=dens)) + geom_line() + ggtitle("uniform density")
p2 <- ggplot(df, aes(x=x, y=dist)) + geom_line() + ggtitle("uniform distribution")
p3 <- ggplot(df, aes(x=x, y=rand)) + geom_point() + ggtitle("random uniform distribution values") + 
  geom_hline(yintercept = 0.25, col = "red", linetype = 3) +
  geom_hline(yintercept = 0.5, col = "red", linetype = 3) +
  geom_hline(yintercept = 0.75, col = "red", linetype = 3)
grid.arrange(p1, p2,p3, ncol=3)
```

In the third plot above, we see that approximately 50\% of the data lies below the median and 50\% lies above it. Hence, if we wanted to sample 80\% of our data, we would expect that proportion to be correct *on average* due to the randomness of the draws. We can demonstrate this as follows: sample 80% of the values of a sequence of numbers from 1 to 20 and take the length of this sequence

```{r}
set.seed(20)
obs_drawn <- vapply(1:2000, function(x) length(seq(1,20,1)[runif(20) < 0.8]), 0)
hist(obs_drawn)
mu20 <- mean(obs_drawn) # += 16
sd20<- sqrt(var(obs_drawn)) # +- 1.76
err <- sqrt(var(obs_drawn)) / 16
```

So on average, we draw 16 observations with an sd of 1.76 or +- 11% error. We can do the same for 100 observations:

```{r}
set.seed(40)
obs_drawn <- vapply(1:2000, function(x) length(seq(1,100,1)[runif(100) < 0.8]), 0)
hist(obs_drawn)
mu100 <- mean(obs_drawn) # += 80
sd100<- sqrt(var(obs_drawn)) # +- 3.96
err100 <- sqrt(var(obs_drawn)) / 80 # 5%
```

On average, this gives us the right number of observations. By the law of large numbers, we expect the 'error' (measured above as the standard deviation divided by the expected sample size) to go down as the sample size $n \to \infty$.  

In the `permutation_test()` function, we can specify whether we want to sample this way using the `use_sample()` flag. As shown above, there is always a chance that we will draw a sample that is extremely few in number of observations. To make sure that this does not happen, we impose the rule that the proportion of observations may not be above or below one standard error away from the proportion.

```{r}
set.seed(888)
pt <- permutation_test(CSFI, TFI, 999, use_sample = FALSE)
pt
```

```{r}
set.seed(888)
pt <- permutation_test(CSFI, TFI, 999, use_sample = TRUE)
pt
```
